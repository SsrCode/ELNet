{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ELNetV2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1wb0cYthA8Owe4yGlmVMurmDD-6ftBZER",
      "authorship_tag": "ABX9TyNpx0Jn4fZ4Htag969dFhmd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SsrCode/ELNet/blob/master/ELNetV2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uoKMFxsITPn",
        "colab_type": "code",
        "outputId": "3aafe2ff-3824-46ee-e7e1-9c7a02b43a06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Apr  7 04:15:21 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.64.00    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P8     7W /  75W |      0MiB /  7611MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mP7CLp-cIfM7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "02a7f4fc-dccb-4536-95f1-3aab6d4361ad"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtlDCXj5Ijoo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9f65d35d-2126-4aac-a98f-ed8068937c77"
      },
      "source": [
        "import torch\n",
        "import math\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyper-parameters\n",
        "num_epochs = 200\n",
        "\n",
        "# Image preprocessing modules\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.Pad(4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32),\n",
        "    #transforms.Resize(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "    ])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    #transforms.Resize(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "# CIFAR-10 dataset\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='/.data',\n",
        "                                             train=True, \n",
        "                                             transform=transform_train,\n",
        "                                             download=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='/.data',\n",
        "                                            train=True, \n",
        "                                            transform=transform_test)\n",
        "\n",
        "# Data loader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=128, \n",
        "                                           shuffle=True,\n",
        "                                           num_workers=2)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=100, \n",
        "                                          shuffle=False,\n",
        "                                          num_workers=2)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czqSollk3EQT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "d83aa580-2496-49bf-80b6-68c63c1a7124"
      },
      "source": [
        "！pwd"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-ac4bfe892df2>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    ！pwd\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDhsKVNwIn-r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _make_divisible(v, divisor, min_value=None):\n",
        "    \"\"\"\n",
        "    This function is taken from the original tf repo.\n",
        "    It ensures that all layers have a channel number that is divisible by 8\n",
        "    It can be seen here:\n",
        "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
        "    \"\"\"\n",
        "    if min_value is None:\n",
        "        min_value = divisor\n",
        "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
        "    # Make sure that round down does not go down by more than 10%.\n",
        "    if new_v < 0.9 * v:\n",
        "        new_v += divisor\n",
        "    return new_v\n",
        "\n",
        "class Hswish(nn.Module):\n",
        "    def __init__(self, inplace=True):\n",
        "        super(Hswish, self).__init__()\n",
        "        self.inplace = inplace\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x * F.relu6(x + 3., inplace=self.inplace) / 6.\n",
        "\n",
        "\n",
        "class Hsigmoid(nn.Module):\n",
        "    def __init__(self, inplace=True):\n",
        "        super(Hsigmoid, self).__init__()\n",
        "        self.inplace = inplace\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.relu6(x + 3., inplace=self.inplace) / 6.\n",
        "\n",
        "\n",
        "class SEModule(nn.Module):\n",
        "    def __init__(self, channel, reduction=4):\n",
        "        super(SEModule, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channel, channel // reduction, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(channel // reduction, channel, bias=False),\n",
        "            Hsigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, _, _ = x.size()\n",
        "        y = self.avg_pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1, 1)\n",
        "        return x * y.expand_as(x)\n",
        "\n",
        "def splitChannels(channels, num_groups):\n",
        "    split_channels = [channels//num_groups for _ in range(num_groups)]\n",
        "    split_channels[0] += channels - sum(split_channels)\n",
        "    return split_channels\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, in_channels,hidden,out_channels,kernel_size,stride,se=True,nl='RE',group=4):\n",
        "        self.hid = hidden\n",
        "        self.inc = in_channels\n",
        "        super(Block,self).__init__()\n",
        "        if nl == 'RE':\n",
        "            nlin_layer = nn.ReLU # or ReLU6\n",
        "        elif nl == 'HS':\n",
        "            nlin_layer = Hswish\n",
        "        self.conv1 = nn.Conv2d(in_channels,hidden,1,stride=1,groups = group)\n",
        "        self.conv2 = nn.Conv2d(hidden,out_channels,1,stride=1,groups = group)\n",
        "\n",
        "        self.operation1 = nn.Sequential(\n",
        "            ##\n",
        "            nn.BatchNorm2d(hidden),\n",
        "            nlin_layer(inplace=True),\n",
        "            #MDConv(in_channels,out_channels,kernel_size=[3,5],stride),\n",
        "            nn.Conv2d(hidden,hidden,kernel_size,stride, kernel_size//2, groups=hidden, bias=False),\n",
        "            nn.BatchNorm2d(hidden),\n",
        "            SEModule(hidden),\n",
        "            nlin_layer(inplace=True)\n",
        "        )\n",
        "        self.operation2 = nn.Sequential(\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nlin_layer(inplace=True)\n",
        "        )\n",
        "\n",
        "        if stride == 1 and in_channels == out_channels:\n",
        "            self.shortcut = nn.Sequential()\n",
        "        else:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels,in_channels,3,stride,1,groups=in_channels, bias=False),\n",
        "                nn.BatchNorm2d(in_channels),\n",
        "                nn.Conv2d(in_channels, out_channels, 1, 1, 0, bias=False),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "                nlin_layer(inplace=True)\n",
        "            )\n",
        "\n",
        "    def forward(self,x):\n",
        "        group1 = self.inc//4\n",
        "        group2 = self.hid//4\n",
        "        inx = torch.split(x, group1, 1)\n",
        "       \n",
        "        for i in range(4):\n",
        "            y = inx[i]\n",
        "            if i == 0:\n",
        "                sp = y\n",
        "                out = sp\n",
        "            else:\n",
        "                sp = sp + y\n",
        "                out = torch.cat((out, sp), 1)\n",
        "        out = self.conv1(out)\n",
        "        out = self.operation1(out)\n",
        "       \n",
        "        inx = torch.split(out, group2, 1)\n",
        "       \n",
        "        for i in range(4):\n",
        "            y = inx[3-i]\n",
        "            if i == 0:\n",
        "                sp = y\n",
        "                out = sp\n",
        "            else:\n",
        "                sp = sp + y\n",
        "                out = torch.cat((sp,out), 1)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.operation2(out)\n",
        "\n",
        "        return out+self.shortcut(x)\n",
        "\n",
        "class MyNet(nn.Module):\n",
        "    def __init__(self, num_classes=10, width_mult=1.):\n",
        "        super(MyNet, self).__init__()\n",
        "        # setting of inverted residual blocks\n",
        "        self.cfgs = [\n",
        "            # k, exp, c,  se,     nl,  s,\n",
        "            [3, 16,  16,  True,  'HS', 1],\n",
        "            [3, 72,  24,  False, 'HS', 2],\n",
        "            [3, 88,  24,  False, 'HS', 1],\n",
        "            [5, 96,  40,  True,  'HS', 2],\n",
        "            [5, 240, 40,  True,  'HS', 1],\n",
        "            [5, 240, 40,  True,  'HS', 1],\n",
        "            [5, 120, 48,  True,  'HS', 1],\n",
        "            [5, 144, 48,  True,  'HS', 1],\n",
        "            [5, 288, 96,  True,  'HS', 2],\n",
        "            [5, 576, 96,  True,  'HS', 1],\n",
        "            [5, 576, 96,  True,  'HS', 1],\n",
        "            ]\n",
        "\n",
        "        # building first layer\n",
        "        output_channel = _make_divisible(16 * width_mult, 4)\n",
        "        layers = [nn.Sequential(\n",
        "            nn.Conv2d(3, output_channel, 3, 1, 1, bias=False),\n",
        "            nn.BatchNorm2d(output_channel),\n",
        "            nn.ReLU(inplace=True)\n",
        "            )]\n",
        "        input_channel = 16\n",
        "\n",
        "        # building inverted residual blocks\n",
        "        block = Block\n",
        "        for k, exp_size, c, se, nl, s in self.cfgs:\n",
        "            output_channel = c\n",
        "            hidden_channel = exp_size\n",
        "            layers.append(block(input_channel, hidden_channel, output_channel, k, s, se, nl))\n",
        "            input_channel = output_channel\n",
        "        self.features = nn.Sequential(*layers)\n",
        "\n",
        "        # building last several layers\n",
        "        output_channel = _make_divisible(exp_size * width_mult, 4)\n",
        "        self.squeeze = nn.Sequential(\n",
        "            nn.Conv2d(input_channel, output_channel, 1, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(output_channel),\n",
        "            Hswish(inplace=True),\n",
        "            nn.AdaptiveAvgPool2d((1, 1)),\n",
        "        )\n",
        "        input_channel = output_channel\n",
        "\n",
        "        output_channel = 1280\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(input_channel, output_channel, bias=False),\n",
        "            nn.BatchNorm1d(output_channel),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(output_channel, num_classes),\n",
        "        )\n",
        "\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.squeeze(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sM2UMm98Irqz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "class GradualWarmupScheduler(_LRScheduler):\n",
        "    \"\"\" Gradually warm-up(increasing) learning rate in optimizer.\n",
        "    Proposed in 'Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour'.\n",
        "    Args:\n",
        "        optimizer (Optimizer): Wrapped optimizer.\n",
        "        multiplier: target learning rate = base lr * multiplier\n",
        "        total_epoch: target learning rate is reached at total_epoch, gradually\n",
        "        after_scheduler: after target_epoch, use this scheduler(eg. ReduceLROnPlateau)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, optimizer, multiplier, total_epoch, after_scheduler=None):\n",
        "        self.multiplier = multiplier\n",
        "        if self.multiplier <= 1.:\n",
        "            raise ValueError('multiplier should be greater than 1.')\n",
        "        self.total_epoch = total_epoch\n",
        "        self.after_scheduler = after_scheduler\n",
        "        self.finished = False\n",
        "        super().__init__(optimizer)\n",
        "\n",
        "    def get_lr(self):\n",
        "        if self.last_epoch > self.total_epoch:\n",
        "            if self.after_scheduler:\n",
        "                if not self.finished:\n",
        "                    self.after_scheduler.base_lrs = [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
        "                    self.finished = True\n",
        "                return self.after_scheduler.get_lr()\n",
        "            return [base_lr * self.multiplier for base_lr in self.base_lrs]\n",
        "\n",
        "        return [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]\n",
        "\n",
        "    def step_ReduceLROnPlateau(self, metrics, epoch=None):\n",
        "        if epoch is None:\n",
        "            epoch = self.last_epoch + 1\n",
        "        self.last_epoch = epoch if epoch != 0 else 1  # ReduceLROnPlateau is called at the end of epoch, whereas others are called at beginning\n",
        "        if self.last_epoch <= self.total_epoch:\n",
        "            warmup_lr = [base_lr * ((self.multiplier - 1.) * self.last_epoch / self.total_epoch + 1.) for base_lr in self.base_lrs]\n",
        "            for param_group, lr in zip(self.optimizer.param_groups, warmup_lr):\n",
        "                param_group['lr'] = lr\n",
        "        else:\n",
        "            if epoch is None:\n",
        "                self.after_scheduler.step(metrics, None)\n",
        "            else:\n",
        "                self.after_scheduler.step(metrics, epoch - self.total_epoch)\n",
        "\n",
        "    def step(self, epoch=None, metrics=None):\n",
        "        if type(self.after_scheduler) != ReduceLROnPlateau:\n",
        "            if self.finished and self.after_scheduler:\n",
        "                if epoch is None:\n",
        "                    self.after_scheduler.step(None)\n",
        "                else:\n",
        "                    self.after_scheduler.step(epoch - self.total_epoch)\n",
        "            else:\n",
        "                return super(GradualWarmupScheduler, self).step(epoch)\n",
        "        else:\n",
        "            self.step_ReduceLROnPlateau(metrics, epoch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u98L3OWvI05k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c610f9db-e47f-48c2-8d58-c8966a1e44b7"
      },
      "source": [
        "model = MyNet().to(device)\n",
        "print('Total params: %.2fM' % (sum(p.numel() for p in model.parameters())/1000000.0))\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.05,momentum = 0.9,weight_decay=4e-5)\n",
        "scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 200)\n",
        "scheduler_warmup = GradualWarmupScheduler(optimizer, multiplier=8, total_epoch=5, after_scheduler=scheduler_cosine)\n",
        "\n",
        "total_step = len(train_loader)\n",
        "def test():\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Accuracy of the model on the test images: {} %'.format(100 * correct / total))\n",
        "        \n",
        "# Train the model\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if (i+1) % 100 == 0:\n",
        "            print (\"Epoch [{}/{}], Step [{}/{}] Loss: {:.4f}\"\n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "            \n",
        "    scheduler_warmup.step()\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        test()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total params: 1.44M\n",
            "Epoch [1/200], Step [100/391] Loss: 2.2323\n",
            "Epoch [1/200], Step [200/391] Loss: 2.5061\n",
            "Epoch [1/200], Step [300/391] Loss: 2.1744\n",
            "Epoch [2/200], Step [100/391] Loss: 1.8185\n",
            "Epoch [2/200], Step [200/391] Loss: 1.6450\n",
            "Epoch [2/200], Step [300/391] Loss: 1.6641\n",
            "Epoch [3/200], Step [100/391] Loss: 1.6299\n",
            "Epoch [3/200], Step [200/391] Loss: 1.5823\n",
            "Epoch [3/200], Step [300/391] Loss: 1.4070\n",
            "Epoch [4/200], Step [100/391] Loss: 1.3996\n",
            "Epoch [4/200], Step [200/391] Loss: 1.4407\n",
            "Epoch [4/200], Step [300/391] Loss: 1.8379\n",
            "Epoch [5/200], Step [100/391] Loss: 1.5263\n",
            "Epoch [5/200], Step [200/391] Loss: 1.2320\n",
            "Epoch [5/200], Step [300/391] Loss: 1.6552\n",
            "Epoch [6/200], Step [100/391] Loss: 1.6621\n",
            "Epoch [6/200], Step [200/391] Loss: 1.5583\n",
            "Epoch [6/200], Step [300/391] Loss: 1.3467\n",
            "Epoch [7/200], Step [100/391] Loss: 1.2393\n",
            "Epoch [7/200], Step [200/391] Loss: 1.2308\n",
            "Epoch [7/200], Step [300/391] Loss: 1.1792\n",
            "Epoch [8/200], Step [100/391] Loss: 1.2978\n",
            "Epoch [8/200], Step [200/391] Loss: 1.0711\n",
            "Epoch [8/200], Step [300/391] Loss: 1.0984\n",
            "Epoch [9/200], Step [100/391] Loss: 1.2423\n",
            "Epoch [9/200], Step [200/391] Loss: 1.2029\n",
            "Epoch [9/200], Step [300/391] Loss: 1.2214\n",
            "Epoch [10/200], Step [100/391] Loss: 0.8769\n",
            "Epoch [10/200], Step [200/391] Loss: 0.9719\n",
            "Epoch [10/200], Step [300/391] Loss: 1.0851\n",
            "Accuracy of the model on the test images: 50.488 %\n",
            "Epoch [11/200], Step [100/391] Loss: 1.1132\n",
            "Epoch [11/200], Step [200/391] Loss: 1.8682\n",
            "Epoch [11/200], Step [300/391] Loss: 1.6818\n",
            "Epoch [12/200], Step [100/391] Loss: 1.3040\n",
            "Epoch [12/200], Step [200/391] Loss: 1.1303\n",
            "Epoch [12/200], Step [300/391] Loss: 1.0069\n",
            "Epoch [13/200], Step [100/391] Loss: 1.2128\n",
            "Epoch [13/200], Step [200/391] Loss: 1.2873\n",
            "Epoch [13/200], Step [300/391] Loss: 0.7924\n",
            "Epoch [14/200], Step [100/391] Loss: 0.9161\n",
            "Epoch [14/200], Step [200/391] Loss: 1.1732\n",
            "Epoch [14/200], Step [300/391] Loss: 1.0547\n",
            "Epoch [15/200], Step [100/391] Loss: 1.0619\n",
            "Epoch [15/200], Step [200/391] Loss: 0.9646\n",
            "Epoch [15/200], Step [300/391] Loss: 0.9507\n",
            "Epoch [16/200], Step [100/391] Loss: 0.9939\n",
            "Epoch [16/200], Step [200/391] Loss: 0.9235\n",
            "Epoch [16/200], Step [300/391] Loss: 0.9548\n",
            "Epoch [17/200], Step [100/391] Loss: 0.9192\n",
            "Epoch [17/200], Step [200/391] Loss: 0.8756\n",
            "Epoch [17/200], Step [300/391] Loss: 0.8885\n",
            "Epoch [18/200], Step [100/391] Loss: 0.9968\n",
            "Epoch [18/200], Step [200/391] Loss: 0.7843\n",
            "Epoch [18/200], Step [300/391] Loss: 0.7577\n",
            "Epoch [19/200], Step [100/391] Loss: 0.8106\n",
            "Epoch [19/200], Step [200/391] Loss: 0.9580\n",
            "Epoch [19/200], Step [300/391] Loss: 0.7907\n",
            "Epoch [20/200], Step [100/391] Loss: 0.7966\n",
            "Epoch [20/200], Step [200/391] Loss: 0.6769\n",
            "Epoch [20/200], Step [300/391] Loss: 0.8913\n",
            "Accuracy of the model on the test images: 75.106 %\n",
            "Epoch [21/200], Step [100/391] Loss: 0.8010\n",
            "Epoch [21/200], Step [200/391] Loss: 0.7304\n",
            "Epoch [21/200], Step [300/391] Loss: 0.7850\n",
            "Epoch [22/200], Step [100/391] Loss: 0.8117\n",
            "Epoch [22/200], Step [200/391] Loss: 0.5991\n",
            "Epoch [22/200], Step [300/391] Loss: 0.8384\n",
            "Epoch [23/200], Step [100/391] Loss: 0.6867\n",
            "Epoch [23/200], Step [200/391] Loss: 0.8677\n",
            "Epoch [23/200], Step [300/391] Loss: 0.6086\n",
            "Epoch [24/200], Step [100/391] Loss: 0.6886\n",
            "Epoch [24/200], Step [200/391] Loss: 0.6970\n",
            "Epoch [24/200], Step [300/391] Loss: 0.6131\n",
            "Epoch [25/200], Step [100/391] Loss: 0.8273\n",
            "Epoch [25/200], Step [200/391] Loss: 0.6289\n",
            "Epoch [25/200], Step [300/391] Loss: 0.6731\n",
            "Epoch [26/200], Step [100/391] Loss: 0.7243\n",
            "Epoch [26/200], Step [200/391] Loss: 0.5639\n",
            "Epoch [26/200], Step [300/391] Loss: 0.6305\n",
            "Epoch [27/200], Step [100/391] Loss: 0.5664\n",
            "Epoch [27/200], Step [200/391] Loss: 0.6130\n",
            "Epoch [27/200], Step [300/391] Loss: 0.5088\n",
            "Epoch [28/200], Step [100/391] Loss: 0.8211\n",
            "Epoch [28/200], Step [200/391] Loss: 0.8022\n",
            "Epoch [28/200], Step [300/391] Loss: 0.6801\n",
            "Epoch [29/200], Step [100/391] Loss: 0.4773\n",
            "Epoch [29/200], Step [200/391] Loss: 0.5701\n",
            "Epoch [29/200], Step [300/391] Loss: 0.7571\n",
            "Epoch [30/200], Step [100/391] Loss: 0.6881\n",
            "Epoch [30/200], Step [200/391] Loss: 0.6283\n",
            "Epoch [30/200], Step [300/391] Loss: 0.6007\n",
            "Accuracy of the model on the test images: 76.708 %\n",
            "Epoch [31/200], Step [100/391] Loss: 0.7024\n",
            "Epoch [31/200], Step [200/391] Loss: 0.5758\n",
            "Epoch [31/200], Step [300/391] Loss: 0.5353\n",
            "Epoch [32/200], Step [100/391] Loss: 0.5806\n",
            "Epoch [32/200], Step [200/391] Loss: 0.6963\n",
            "Epoch [32/200], Step [300/391] Loss: 0.7975\n",
            "Epoch [33/200], Step [100/391] Loss: 0.6723\n",
            "Epoch [33/200], Step [200/391] Loss: 0.7557\n",
            "Epoch [33/200], Step [300/391] Loss: 0.7172\n",
            "Epoch [34/200], Step [100/391] Loss: 0.5339\n",
            "Epoch [34/200], Step [200/391] Loss: 0.6216\n",
            "Epoch [34/200], Step [300/391] Loss: 0.5847\n",
            "Epoch [35/200], Step [100/391] Loss: 0.4713\n",
            "Epoch [35/200], Step [200/391] Loss: 0.6706\n",
            "Epoch [35/200], Step [300/391] Loss: 0.7343\n",
            "Epoch [36/200], Step [100/391] Loss: 0.5097\n",
            "Epoch [36/200], Step [200/391] Loss: 0.3953\n",
            "Epoch [36/200], Step [300/391] Loss: 0.4710\n",
            "Epoch [37/200], Step [100/391] Loss: 0.5742\n",
            "Epoch [37/200], Step [200/391] Loss: 0.7233\n",
            "Epoch [37/200], Step [300/391] Loss: 0.5192\n",
            "Epoch [38/200], Step [100/391] Loss: 0.4982\n",
            "Epoch [38/200], Step [200/391] Loss: 0.5151\n",
            "Epoch [38/200], Step [300/391] Loss: 0.3477\n",
            "Epoch [39/200], Step [100/391] Loss: 0.7801\n",
            "Epoch [39/200], Step [200/391] Loss: 0.5761\n",
            "Epoch [39/200], Step [300/391] Loss: 0.5682\n",
            "Epoch [40/200], Step [100/391] Loss: 0.5869\n",
            "Epoch [40/200], Step [200/391] Loss: 0.4639\n",
            "Epoch [40/200], Step [300/391] Loss: 0.4734\n",
            "Accuracy of the model on the test images: 81.9 %\n",
            "Epoch [41/200], Step [100/391] Loss: 0.3956\n",
            "Epoch [41/200], Step [200/391] Loss: 0.5806\n",
            "Epoch [41/200], Step [300/391] Loss: 0.4829\n",
            "Epoch [42/200], Step [100/391] Loss: 0.4507\n",
            "Epoch [42/200], Step [200/391] Loss: 0.4928\n",
            "Epoch [42/200], Step [300/391] Loss: 0.8921\n",
            "Epoch [43/200], Step [100/391] Loss: 0.3818\n",
            "Epoch [43/200], Step [200/391] Loss: 0.4680\n",
            "Epoch [43/200], Step [300/391] Loss: 0.4420\n",
            "Epoch [44/200], Step [100/391] Loss: 0.4492\n",
            "Epoch [44/200], Step [200/391] Loss: 0.5521\n",
            "Epoch [44/200], Step [300/391] Loss: 0.4236\n",
            "Epoch [45/200], Step [100/391] Loss: 0.4798\n",
            "Epoch [45/200], Step [200/391] Loss: 0.7299\n",
            "Epoch [45/200], Step [300/391] Loss: 0.5640\n",
            "Epoch [46/200], Step [100/391] Loss: 0.4172\n",
            "Epoch [46/200], Step [200/391] Loss: 0.3073\n",
            "Epoch [46/200], Step [300/391] Loss: 0.5171\n",
            "Epoch [47/200], Step [100/391] Loss: 0.4029\n",
            "Epoch [47/200], Step [200/391] Loss: 0.5643\n",
            "Epoch [47/200], Step [300/391] Loss: 0.5742\n",
            "Epoch [48/200], Step [100/391] Loss: 0.4215\n",
            "Epoch [48/200], Step [200/391] Loss: 0.6270\n",
            "Epoch [48/200], Step [300/391] Loss: 0.7605\n",
            "Epoch [49/200], Step [100/391] Loss: 0.5006\n",
            "Epoch [49/200], Step [200/391] Loss: 0.4381\n",
            "Epoch [49/200], Step [300/391] Loss: 0.3413\n",
            "Epoch [50/200], Step [100/391] Loss: 0.4963\n",
            "Epoch [50/200], Step [200/391] Loss: 0.4912\n",
            "Epoch [50/200], Step [300/391] Loss: 0.4321\n",
            "Accuracy of the model on the test images: 85.734 %\n",
            "Epoch [51/200], Step [100/391] Loss: 0.4016\n",
            "Epoch [51/200], Step [200/391] Loss: 0.4881\n",
            "Epoch [51/200], Step [300/391] Loss: 0.4649\n",
            "Epoch [52/200], Step [100/391] Loss: 0.5354\n",
            "Epoch [52/200], Step [200/391] Loss: 0.5525\n",
            "Epoch [52/200], Step [300/391] Loss: 0.4412\n",
            "Epoch [53/200], Step [100/391] Loss: 0.5219\n",
            "Epoch [53/200], Step [200/391] Loss: 0.3845\n",
            "Epoch [53/200], Step [300/391] Loss: 0.5112\n",
            "Epoch [54/200], Step [100/391] Loss: 0.5377\n",
            "Epoch [54/200], Step [200/391] Loss: 0.3924\n",
            "Epoch [54/200], Step [300/391] Loss: 0.3736\n",
            "Epoch [55/200], Step [100/391] Loss: 0.4539\n",
            "Epoch [55/200], Step [200/391] Loss: 0.5035\n",
            "Epoch [55/200], Step [300/391] Loss: 0.3629\n",
            "Epoch [56/200], Step [100/391] Loss: 0.4602\n",
            "Epoch [56/200], Step [200/391] Loss: 0.5082\n",
            "Epoch [56/200], Step [300/391] Loss: 0.4222\n",
            "Epoch [57/200], Step [100/391] Loss: 0.4513\n",
            "Epoch [57/200], Step [200/391] Loss: 0.4156\n",
            "Epoch [57/200], Step [300/391] Loss: 0.4104\n",
            "Epoch [58/200], Step [100/391] Loss: 0.4413\n",
            "Epoch [58/200], Step [200/391] Loss: 0.4286\n",
            "Epoch [58/200], Step [300/391] Loss: 0.4189\n",
            "Epoch [59/200], Step [100/391] Loss: 0.5526\n",
            "Epoch [59/200], Step [200/391] Loss: 0.4801\n",
            "Epoch [59/200], Step [300/391] Loss: 0.4326\n",
            "Epoch [60/200], Step [100/391] Loss: 0.4851\n",
            "Epoch [60/200], Step [200/391] Loss: 0.3906\n",
            "Epoch [60/200], Step [300/391] Loss: 0.6359\n",
            "Accuracy of the model on the test images: 86.832 %\n",
            "Epoch [61/200], Step [100/391] Loss: 0.3228\n",
            "Epoch [61/200], Step [200/391] Loss: 0.3941\n",
            "Epoch [61/200], Step [300/391] Loss: 0.3724\n",
            "Epoch [62/200], Step [100/391] Loss: 0.4932\n",
            "Epoch [62/200], Step [200/391] Loss: 0.3452\n",
            "Epoch [62/200], Step [300/391] Loss: 0.4156\n",
            "Epoch [63/200], Step [100/391] Loss: 0.4457\n",
            "Epoch [63/200], Step [200/391] Loss: 0.4632\n",
            "Epoch [63/200], Step [300/391] Loss: 0.4813\n",
            "Epoch [64/200], Step [100/391] Loss: 0.2589\n",
            "Epoch [64/200], Step [200/391] Loss: 0.3078\n",
            "Epoch [64/200], Step [300/391] Loss: 0.4071\n",
            "Epoch [65/200], Step [100/391] Loss: 0.5171\n",
            "Epoch [65/200], Step [200/391] Loss: 0.4072\n",
            "Epoch [65/200], Step [300/391] Loss: 0.4085\n",
            "Epoch [66/200], Step [100/391] Loss: 0.4089\n",
            "Epoch [66/200], Step [200/391] Loss: 0.4824\n",
            "Epoch [66/200], Step [300/391] Loss: 0.3467\n",
            "Epoch [67/200], Step [100/391] Loss: 0.2408\n",
            "Epoch [67/200], Step [200/391] Loss: 0.3419\n",
            "Epoch [67/200], Step [300/391] Loss: 0.4082\n",
            "Epoch [68/200], Step [100/391] Loss: 0.3383\n",
            "Epoch [68/200], Step [200/391] Loss: 0.2867\n",
            "Epoch [68/200], Step [300/391] Loss: 0.3951\n",
            "Epoch [69/200], Step [100/391] Loss: 0.2737\n",
            "Epoch [69/200], Step [200/391] Loss: 0.3575\n",
            "Epoch [69/200], Step [300/391] Loss: 0.4940\n",
            "Epoch [70/200], Step [100/391] Loss: 0.3153\n",
            "Epoch [70/200], Step [200/391] Loss: 0.2585\n",
            "Epoch [70/200], Step [300/391] Loss: 0.4077\n",
            "Accuracy of the model on the test images: 86.746 %\n",
            "Epoch [71/200], Step [100/391] Loss: 0.3739\n",
            "Epoch [71/200], Step [200/391] Loss: 0.5794\n",
            "Epoch [71/200], Step [300/391] Loss: 0.3238\n",
            "Epoch [72/200], Step [100/391] Loss: 0.4882\n",
            "Epoch [72/200], Step [200/391] Loss: 0.2567\n",
            "Epoch [72/200], Step [300/391] Loss: 0.4340\n",
            "Epoch [73/200], Step [100/391] Loss: 0.3603\n",
            "Epoch [73/200], Step [200/391] Loss: 0.3549\n",
            "Epoch [73/200], Step [300/391] Loss: 0.3954\n",
            "Epoch [74/200], Step [100/391] Loss: 0.2897\n",
            "Epoch [74/200], Step [200/391] Loss: 0.3622\n",
            "Epoch [74/200], Step [300/391] Loss: 0.2152\n",
            "Epoch [75/200], Step [100/391] Loss: 0.3852\n",
            "Epoch [75/200], Step [200/391] Loss: 0.2913\n",
            "Epoch [75/200], Step [300/391] Loss: 0.4161\n",
            "Epoch [76/200], Step [100/391] Loss: 0.3677\n",
            "Epoch [76/200], Step [200/391] Loss: 0.4553\n",
            "Epoch [76/200], Step [300/391] Loss: 0.2621\n",
            "Epoch [77/200], Step [100/391] Loss: 0.2985\n",
            "Epoch [77/200], Step [200/391] Loss: 0.4032\n",
            "Epoch [77/200], Step [300/391] Loss: 0.3953\n",
            "Epoch [78/200], Step [100/391] Loss: 0.3271\n",
            "Epoch [78/200], Step [200/391] Loss: 0.3525\n",
            "Epoch [78/200], Step [300/391] Loss: 0.2122\n",
            "Epoch [79/200], Step [100/391] Loss: 0.4142\n",
            "Epoch [79/200], Step [200/391] Loss: 0.3492\n",
            "Epoch [79/200], Step [300/391] Loss: 0.3957\n",
            "Epoch [80/200], Step [100/391] Loss: 0.2766\n",
            "Epoch [80/200], Step [200/391] Loss: 0.2900\n",
            "Epoch [80/200], Step [300/391] Loss: 0.4208\n",
            "Accuracy of the model on the test images: 88.016 %\n",
            "Epoch [81/200], Step [100/391] Loss: 0.4594\n",
            "Epoch [81/200], Step [200/391] Loss: 0.3051\n",
            "Epoch [81/200], Step [300/391] Loss: 0.3261\n",
            "Epoch [82/200], Step [100/391] Loss: 0.2732\n",
            "Epoch [82/200], Step [200/391] Loss: 0.3968\n",
            "Epoch [82/200], Step [300/391] Loss: 0.3662\n",
            "Epoch [83/200], Step [100/391] Loss: 0.3994\n",
            "Epoch [83/200], Step [200/391] Loss: 0.3038\n",
            "Epoch [83/200], Step [300/391] Loss: 0.3393\n",
            "Epoch [84/200], Step [100/391] Loss: 0.3984\n",
            "Epoch [84/200], Step [200/391] Loss: 0.3500\n",
            "Epoch [84/200], Step [300/391] Loss: 0.3884\n",
            "Epoch [85/200], Step [100/391] Loss: 0.3884\n",
            "Epoch [85/200], Step [200/391] Loss: 0.4654\n",
            "Epoch [85/200], Step [300/391] Loss: 0.3704\n",
            "Epoch [86/200], Step [100/391] Loss: 0.1934\n",
            "Epoch [86/200], Step [200/391] Loss: 0.2223\n",
            "Epoch [86/200], Step [300/391] Loss: 0.3228\n",
            "Epoch [87/200], Step [100/391] Loss: 0.2474\n",
            "Epoch [87/200], Step [200/391] Loss: 0.2900\n",
            "Epoch [87/200], Step [300/391] Loss: 0.4388\n",
            "Epoch [88/200], Step [100/391] Loss: 0.2849\n",
            "Epoch [88/200], Step [200/391] Loss: 0.3932\n",
            "Epoch [88/200], Step [300/391] Loss: 0.3338\n",
            "Epoch [89/200], Step [100/391] Loss: 0.3954\n",
            "Epoch [89/200], Step [200/391] Loss: 0.1846\n",
            "Epoch [89/200], Step [300/391] Loss: 0.3774\n",
            "Epoch [90/200], Step [100/391] Loss: 0.3332\n",
            "Epoch [90/200], Step [200/391] Loss: 0.1950\n",
            "Epoch [90/200], Step [300/391] Loss: 0.3283\n",
            "Accuracy of the model on the test images: 90.148 %\n",
            "Epoch [91/200], Step [100/391] Loss: 0.3977\n",
            "Epoch [91/200], Step [200/391] Loss: 0.2567\n",
            "Epoch [91/200], Step [300/391] Loss: 0.2114\n",
            "Epoch [92/200], Step [100/391] Loss: 0.4522\n",
            "Epoch [92/200], Step [200/391] Loss: 0.2268\n",
            "Epoch [92/200], Step [300/391] Loss: 0.3664\n",
            "Epoch [93/200], Step [100/391] Loss: 0.2640\n",
            "Epoch [93/200], Step [200/391] Loss: 0.3458\n",
            "Epoch [93/200], Step [300/391] Loss: 0.3084\n",
            "Epoch [94/200], Step [100/391] Loss: 0.3720\n",
            "Epoch [94/200], Step [200/391] Loss: 0.2120\n",
            "Epoch [94/200], Step [300/391] Loss: 0.2585\n",
            "Epoch [95/200], Step [100/391] Loss: 0.2276\n",
            "Epoch [95/200], Step [200/391] Loss: 0.3448\n",
            "Epoch [95/200], Step [300/391] Loss: 0.3028\n",
            "Epoch [96/200], Step [100/391] Loss: 0.2201\n",
            "Epoch [96/200], Step [200/391] Loss: 0.2488\n",
            "Epoch [96/200], Step [300/391] Loss: 0.2197\n",
            "Epoch [97/200], Step [100/391] Loss: 0.1974\n",
            "Epoch [97/200], Step [200/391] Loss: 0.2772\n",
            "Epoch [97/200], Step [300/391] Loss: 0.3369\n",
            "Epoch [98/200], Step [100/391] Loss: 0.2350\n",
            "Epoch [98/200], Step [200/391] Loss: 0.2576\n",
            "Epoch [98/200], Step [300/391] Loss: 0.3026\n",
            "Epoch [99/200], Step [100/391] Loss: 0.2475\n",
            "Epoch [99/200], Step [200/391] Loss: 0.3646\n",
            "Epoch [99/200], Step [300/391] Loss: 0.2911\n",
            "Epoch [100/200], Step [100/391] Loss: 0.2347\n",
            "Epoch [100/200], Step [200/391] Loss: 0.2728\n",
            "Epoch [100/200], Step [300/391] Loss: 0.2186\n",
            "Accuracy of the model on the test images: 92.12 %\n",
            "Epoch [101/200], Step [100/391] Loss: 0.2249\n",
            "Epoch [101/200], Step [200/391] Loss: 0.1865\n",
            "Epoch [101/200], Step [300/391] Loss: 0.2537\n",
            "Epoch [102/200], Step [100/391] Loss: 0.2354\n",
            "Epoch [102/200], Step [200/391] Loss: 0.4401\n",
            "Epoch [102/200], Step [300/391] Loss: 0.1755\n",
            "Epoch [103/200], Step [100/391] Loss: 0.3109\n",
            "Epoch [103/200], Step [200/391] Loss: 0.2762\n",
            "Epoch [103/200], Step [300/391] Loss: 0.3719\n",
            "Epoch [104/200], Step [100/391] Loss: 0.2088\n",
            "Epoch [104/200], Step [200/391] Loss: 0.2154\n",
            "Epoch [104/200], Step [300/391] Loss: 0.1964\n",
            "Epoch [105/200], Step [100/391] Loss: 0.1578\n",
            "Epoch [105/200], Step [200/391] Loss: 0.2376\n",
            "Epoch [105/200], Step [300/391] Loss: 0.3197\n",
            "Epoch [106/200], Step [100/391] Loss: 0.2192\n",
            "Epoch [106/200], Step [200/391] Loss: 0.2409\n",
            "Epoch [106/200], Step [300/391] Loss: 0.3526\n",
            "Epoch [107/200], Step [100/391] Loss: 0.2260\n",
            "Epoch [107/200], Step [200/391] Loss: 0.2536\n",
            "Epoch [107/200], Step [300/391] Loss: 0.2273\n",
            "Epoch [108/200], Step [100/391] Loss: 0.1873\n",
            "Epoch [108/200], Step [200/391] Loss: 0.3461\n",
            "Epoch [108/200], Step [300/391] Loss: 0.1840\n",
            "Epoch [109/200], Step [100/391] Loss: 0.2337\n",
            "Epoch [109/200], Step [200/391] Loss: 0.2064\n",
            "Epoch [109/200], Step [300/391] Loss: 0.2009\n",
            "Epoch [110/200], Step [100/391] Loss: 0.1803\n",
            "Epoch [110/200], Step [200/391] Loss: 0.1991\n",
            "Epoch [110/200], Step [300/391] Loss: 0.2670\n",
            "Accuracy of the model on the test images: 93.682 %\n",
            "Epoch [111/200], Step [100/391] Loss: 0.2410\n",
            "Epoch [111/200], Step [200/391] Loss: 0.1280\n",
            "Epoch [111/200], Step [300/391] Loss: 0.2114\n",
            "Epoch [112/200], Step [100/391] Loss: 0.2393\n",
            "Epoch [112/200], Step [200/391] Loss: 0.2580\n",
            "Epoch [112/200], Step [300/391] Loss: 0.1879\n",
            "Epoch [113/200], Step [100/391] Loss: 0.2198\n",
            "Epoch [113/200], Step [200/391] Loss: 0.1903\n",
            "Epoch [113/200], Step [300/391] Loss: 0.2150\n",
            "Epoch [114/200], Step [100/391] Loss: 0.1151\n",
            "Epoch [114/200], Step [200/391] Loss: 0.1920\n",
            "Epoch [114/200], Step [300/391] Loss: 0.2799\n",
            "Epoch [115/200], Step [100/391] Loss: 0.1687\n",
            "Epoch [115/200], Step [200/391] Loss: 0.2644\n",
            "Epoch [115/200], Step [300/391] Loss: 0.1327\n",
            "Epoch [116/200], Step [100/391] Loss: 0.1523\n",
            "Epoch [116/200], Step [200/391] Loss: 0.2375\n",
            "Epoch [116/200], Step [300/391] Loss: 0.2966\n",
            "Epoch [117/200], Step [100/391] Loss: 0.2272\n",
            "Epoch [117/200], Step [200/391] Loss: 0.1706\n",
            "Epoch [117/200], Step [300/391] Loss: 0.1819\n",
            "Epoch [118/200], Step [100/391] Loss: 0.1991\n",
            "Epoch [118/200], Step [200/391] Loss: 0.2294\n",
            "Epoch [118/200], Step [300/391] Loss: 0.1444\n",
            "Epoch [119/200], Step [100/391] Loss: 0.2157\n",
            "Epoch [119/200], Step [200/391] Loss: 0.1633\n",
            "Epoch [119/200], Step [300/391] Loss: 0.2349\n",
            "Epoch [120/200], Step [100/391] Loss: 0.2362\n",
            "Epoch [120/200], Step [200/391] Loss: 0.1405\n",
            "Epoch [120/200], Step [300/391] Loss: 0.1793\n",
            "Accuracy of the model on the test images: 95.034 %\n",
            "Epoch [121/200], Step [100/391] Loss: 0.1388\n",
            "Epoch [121/200], Step [200/391] Loss: 0.1822\n",
            "Epoch [121/200], Step [300/391] Loss: 0.1447\n",
            "Epoch [122/200], Step [100/391] Loss: 0.2328\n",
            "Epoch [122/200], Step [200/391] Loss: 0.1972\n",
            "Epoch [122/200], Step [300/391] Loss: 0.2217\n",
            "Epoch [123/200], Step [100/391] Loss: 0.2701\n",
            "Epoch [123/200], Step [200/391] Loss: 0.1801\n",
            "Epoch [123/200], Step [300/391] Loss: 0.1968\n",
            "Epoch [124/200], Step [100/391] Loss: 0.3646\n",
            "Epoch [124/200], Step [200/391] Loss: 0.2303\n",
            "Epoch [124/200], Step [300/391] Loss: 0.1756\n",
            "Epoch [125/200], Step [100/391] Loss: 0.2242\n",
            "Epoch [125/200], Step [200/391] Loss: 0.1458\n",
            "Epoch [125/200], Step [300/391] Loss: 0.1463\n",
            "Epoch [126/200], Step [100/391] Loss: 0.1234\n",
            "Epoch [126/200], Step [200/391] Loss: 0.1960\n",
            "Epoch [126/200], Step [300/391] Loss: 0.1866\n",
            "Epoch [127/200], Step [100/391] Loss: 0.0699\n",
            "Epoch [127/200], Step [200/391] Loss: 0.0708\n",
            "Epoch [127/200], Step [300/391] Loss: 0.2520\n",
            "Epoch [128/200], Step [100/391] Loss: 0.1579\n",
            "Epoch [128/200], Step [200/391] Loss: 0.2239\n",
            "Epoch [128/200], Step [300/391] Loss: 0.2389\n",
            "Epoch [129/200], Step [100/391] Loss: 0.0793\n",
            "Epoch [129/200], Step [200/391] Loss: 0.1036\n",
            "Epoch [129/200], Step [300/391] Loss: 0.1335\n",
            "Epoch [130/200], Step [100/391] Loss: 0.2328\n",
            "Epoch [130/200], Step [200/391] Loss: 0.0989\n",
            "Epoch [130/200], Step [300/391] Loss: 0.1373\n",
            "Accuracy of the model on the test images: 95.094 %\n",
            "Epoch [131/200], Step [100/391] Loss: 0.1220\n",
            "Epoch [131/200], Step [200/391] Loss: 0.1620\n",
            "Epoch [131/200], Step [300/391] Loss: 0.1134\n",
            "Epoch [132/200], Step [100/391] Loss: 0.1220\n",
            "Epoch [132/200], Step [200/391] Loss: 0.0828\n",
            "Epoch [132/200], Step [300/391] Loss: 0.1771\n",
            "Epoch [133/200], Step [100/391] Loss: 0.1287\n",
            "Epoch [133/200], Step [200/391] Loss: 0.1073\n",
            "Epoch [133/200], Step [300/391] Loss: 0.1153\n",
            "Epoch [134/200], Step [100/391] Loss: 0.1388\n",
            "Epoch [134/200], Step [200/391] Loss: 0.2131\n",
            "Epoch [134/200], Step [300/391] Loss: 0.1698\n",
            "Epoch [135/200], Step [100/391] Loss: 0.1171\n",
            "Epoch [135/200], Step [200/391] Loss: 0.0873\n",
            "Epoch [135/200], Step [300/391] Loss: 0.1197\n",
            "Epoch [136/200], Step [100/391] Loss: 0.1087\n",
            "Epoch [136/200], Step [200/391] Loss: 0.1332\n",
            "Epoch [136/200], Step [300/391] Loss: 0.1715\n",
            "Epoch [137/200], Step [100/391] Loss: 0.1086\n",
            "Epoch [137/200], Step [200/391] Loss: 0.1384\n",
            "Epoch [137/200], Step [300/391] Loss: 0.2144\n",
            "Epoch [138/200], Step [100/391] Loss: 0.1836\n",
            "Epoch [138/200], Step [200/391] Loss: 0.0718\n",
            "Epoch [138/200], Step [300/391] Loss: 0.1363\n",
            "Epoch [139/200], Step [100/391] Loss: 0.1173\n",
            "Epoch [139/200], Step [200/391] Loss: 0.1415\n",
            "Epoch [139/200], Step [300/391] Loss: 0.1229\n",
            "Epoch [140/200], Step [100/391] Loss: 0.1218\n",
            "Epoch [140/200], Step [200/391] Loss: 0.1339\n",
            "Epoch [140/200], Step [300/391] Loss: 0.1570\n",
            "Accuracy of the model on the test images: 97.18 %\n",
            "Epoch [141/200], Step [100/391] Loss: 0.0842\n",
            "Epoch [141/200], Step [200/391] Loss: 0.1201\n",
            "Epoch [141/200], Step [300/391] Loss: 0.1737\n",
            "Epoch [142/200], Step [100/391] Loss: 0.0676\n",
            "Epoch [142/200], Step [200/391] Loss: 0.1050\n",
            "Epoch [142/200], Step [300/391] Loss: 0.2214\n",
            "Epoch [143/200], Step [100/391] Loss: 0.1038\n",
            "Epoch [143/200], Step [200/391] Loss: 0.1498\n",
            "Epoch [143/200], Step [300/391] Loss: 0.1319\n",
            "Epoch [144/200], Step [100/391] Loss: 0.1763\n",
            "Epoch [144/200], Step [200/391] Loss: 0.0416\n",
            "Epoch [144/200], Step [300/391] Loss: 0.0884\n",
            "Epoch [145/200], Step [100/391] Loss: 0.0912\n",
            "Epoch [145/200], Step [200/391] Loss: 0.0496\n",
            "Epoch [145/200], Step [300/391] Loss: 0.0855\n",
            "Epoch [146/200], Step [100/391] Loss: 0.0995\n",
            "Epoch [146/200], Step [200/391] Loss: 0.0563\n",
            "Epoch [146/200], Step [300/391] Loss: 0.0531\n",
            "Epoch [147/200], Step [100/391] Loss: 0.1174\n",
            "Epoch [147/200], Step [200/391] Loss: 0.0589\n",
            "Epoch [147/200], Step [300/391] Loss: 0.0449\n",
            "Epoch [148/200], Step [100/391] Loss: 0.0390\n",
            "Epoch [148/200], Step [200/391] Loss: 0.0967\n",
            "Epoch [148/200], Step [300/391] Loss: 0.1160\n",
            "Epoch [149/200], Step [100/391] Loss: 0.0565\n",
            "Epoch [149/200], Step [200/391] Loss: 0.1195\n",
            "Epoch [149/200], Step [300/391] Loss: 0.0711\n",
            "Epoch [150/200], Step [100/391] Loss: 0.0475\n",
            "Epoch [150/200], Step [200/391] Loss: 0.0695\n",
            "Epoch [150/200], Step [300/391] Loss: 0.0643\n",
            "Accuracy of the model on the test images: 98.486 %\n",
            "Epoch [151/200], Step [100/391] Loss: 0.0661\n",
            "Epoch [151/200], Step [200/391] Loss: 0.0469\n",
            "Epoch [151/200], Step [300/391] Loss: 0.1170\n",
            "Epoch [152/200], Step [100/391] Loss: 0.0225\n",
            "Epoch [152/200], Step [200/391] Loss: 0.0557\n",
            "Epoch [152/200], Step [300/391] Loss: 0.1014\n",
            "Epoch [153/200], Step [100/391] Loss: 0.0510\n",
            "Epoch [153/200], Step [200/391] Loss: 0.0305\n",
            "Epoch [153/200], Step [300/391] Loss: 0.0696\n",
            "Epoch [154/200], Step [100/391] Loss: 0.0273\n",
            "Epoch [154/200], Step [200/391] Loss: 0.0499\n",
            "Epoch [154/200], Step [300/391] Loss: 0.1140\n",
            "Epoch [155/200], Step [100/391] Loss: 0.0996\n",
            "Epoch [155/200], Step [200/391] Loss: 0.0596\n",
            "Epoch [155/200], Step [300/391] Loss: 0.0309\n",
            "Epoch [156/200], Step [100/391] Loss: 0.0838\n",
            "Epoch [156/200], Step [200/391] Loss: 0.0974\n",
            "Epoch [156/200], Step [300/391] Loss: 0.0598\n",
            "Epoch [157/200], Step [100/391] Loss: 0.0599\n",
            "Epoch [157/200], Step [200/391] Loss: 0.0533\n",
            "Epoch [157/200], Step [300/391] Loss: 0.0885\n",
            "Epoch [158/200], Step [100/391] Loss: 0.0559\n",
            "Epoch [158/200], Step [200/391] Loss: 0.0354\n",
            "Epoch [158/200], Step [300/391] Loss: 0.0640\n",
            "Epoch [159/200], Step [100/391] Loss: 0.0326\n",
            "Epoch [159/200], Step [200/391] Loss: 0.0472\n",
            "Epoch [159/200], Step [300/391] Loss: 0.0618\n",
            "Epoch [160/200], Step [100/391] Loss: 0.0152\n",
            "Epoch [160/200], Step [200/391] Loss: 0.0545\n",
            "Epoch [160/200], Step [300/391] Loss: 0.0368\n",
            "Accuracy of the model on the test images: 99.068 %\n",
            "Epoch [161/200], Step [100/391] Loss: 0.0301\n",
            "Epoch [161/200], Step [200/391] Loss: 0.0145\n",
            "Epoch [161/200], Step [300/391] Loss: 0.0700\n",
            "Epoch [162/200], Step [100/391] Loss: 0.0488\n",
            "Epoch [162/200], Step [200/391] Loss: 0.0328\n",
            "Epoch [162/200], Step [300/391] Loss: 0.0689\n",
            "Epoch [163/200], Step [100/391] Loss: 0.0325\n",
            "Epoch [163/200], Step [200/391] Loss: 0.0185\n",
            "Epoch [163/200], Step [300/391] Loss: 0.0834\n",
            "Epoch [164/200], Step [100/391] Loss: 0.0709\n",
            "Epoch [164/200], Step [200/391] Loss: 0.0343\n",
            "Epoch [164/200], Step [300/391] Loss: 0.0288\n",
            "Epoch [165/200], Step [100/391] Loss: 0.0139\n",
            "Epoch [165/200], Step [200/391] Loss: 0.0189\n",
            "Epoch [165/200], Step [300/391] Loss: 0.0232\n",
            "Epoch [166/200], Step [100/391] Loss: 0.0322\n",
            "Epoch [166/200], Step [200/391] Loss: 0.0336\n",
            "Epoch [166/200], Step [300/391] Loss: 0.0253\n",
            "Epoch [167/200], Step [100/391] Loss: 0.0053\n",
            "Epoch [167/200], Step [200/391] Loss: 0.0247\n",
            "Epoch [167/200], Step [300/391] Loss: 0.0293\n",
            "Epoch [168/200], Step [100/391] Loss: 0.0162\n",
            "Epoch [168/200], Step [200/391] Loss: 0.0173\n",
            "Epoch [168/200], Step [300/391] Loss: 0.0332\n",
            "Epoch [169/200], Step [100/391] Loss: 0.0428\n",
            "Epoch [169/200], Step [200/391] Loss: 0.0216\n",
            "Epoch [169/200], Step [300/391] Loss: 0.0255\n",
            "Epoch [170/200], Step [100/391] Loss: 0.0258\n",
            "Epoch [170/200], Step [200/391] Loss: 0.0408\n",
            "Epoch [170/200], Step [300/391] Loss: 0.0186\n",
            "Accuracy of the model on the test images: 99.786 %\n",
            "Epoch [171/200], Step [100/391] Loss: 0.0069\n",
            "Epoch [171/200], Step [200/391] Loss: 0.0047\n",
            "Epoch [171/200], Step [300/391] Loss: 0.0184\n",
            "Epoch [172/200], Step [100/391] Loss: 0.0247\n",
            "Epoch [172/200], Step [200/391] Loss: 0.0140\n",
            "Epoch [172/200], Step [300/391] Loss: 0.0065\n",
            "Epoch [173/200], Step [100/391] Loss: 0.0103\n",
            "Epoch [173/200], Step [200/391] Loss: 0.0163\n",
            "Epoch [173/200], Step [300/391] Loss: 0.0041\n",
            "Epoch [174/200], Step [100/391] Loss: 0.0047\n",
            "Epoch [174/200], Step [200/391] Loss: 0.0308\n",
            "Epoch [174/200], Step [300/391] Loss: 0.0036\n",
            "Epoch [175/200], Step [100/391] Loss: 0.0324\n",
            "Epoch [175/200], Step [200/391] Loss: 0.0071\n",
            "Epoch [175/200], Step [300/391] Loss: 0.0024\n",
            "Epoch [176/200], Step [100/391] Loss: 0.0022\n",
            "Epoch [176/200], Step [200/391] Loss: 0.0352\n",
            "Epoch [176/200], Step [300/391] Loss: 0.0028\n",
            "Epoch [177/200], Step [100/391] Loss: 0.0132\n",
            "Epoch [177/200], Step [200/391] Loss: 0.0277\n",
            "Epoch [177/200], Step [300/391] Loss: 0.0178\n",
            "Epoch [178/200], Step [100/391] Loss: 0.0189\n",
            "Epoch [178/200], Step [200/391] Loss: 0.0553\n",
            "Epoch [178/200], Step [300/391] Loss: 0.0278\n",
            "Epoch [179/200], Step [100/391] Loss: 0.0015\n",
            "Epoch [179/200], Step [200/391] Loss: 0.0069\n",
            "Epoch [179/200], Step [300/391] Loss: 0.0314\n",
            "Epoch [180/200], Step [100/391] Loss: 0.0029\n",
            "Epoch [180/200], Step [200/391] Loss: 0.0089\n",
            "Epoch [180/200], Step [300/391] Loss: 0.0171\n",
            "Accuracy of the model on the test images: 99.886 %\n",
            "Epoch [181/200], Step [100/391] Loss: 0.0312\n",
            "Epoch [181/200], Step [200/391] Loss: 0.0087\n",
            "Epoch [181/200], Step [300/391] Loss: 0.0190\n",
            "Epoch [182/200], Step [100/391] Loss: 0.0141\n",
            "Epoch [182/200], Step [200/391] Loss: 0.0041\n",
            "Epoch [182/200], Step [300/391] Loss: 0.0455\n",
            "Epoch [183/200], Step [100/391] Loss: 0.0040\n",
            "Epoch [183/200], Step [200/391] Loss: 0.0199\n",
            "Epoch [183/200], Step [300/391] Loss: 0.0036\n",
            "Epoch [184/200], Step [100/391] Loss: 0.0215\n",
            "Epoch [184/200], Step [200/391] Loss: 0.0013\n",
            "Epoch [184/200], Step [300/391] Loss: 0.0162\n",
            "Epoch [185/200], Step [100/391] Loss: 0.0034\n",
            "Epoch [185/200], Step [200/391] Loss: 0.0036\n",
            "Epoch [185/200], Step [300/391] Loss: 0.0186\n",
            "Epoch [186/200], Step [100/391] Loss: 0.0058\n",
            "Epoch [186/200], Step [200/391] Loss: 0.0044\n",
            "Epoch [186/200], Step [300/391] Loss: 0.0159\n",
            "Epoch [187/200], Step [100/391] Loss: 0.0106\n",
            "Epoch [187/200], Step [200/391] Loss: 0.0028\n",
            "Epoch [187/200], Step [300/391] Loss: 0.0028\n",
            "Epoch [188/200], Step [100/391] Loss: 0.0033\n",
            "Epoch [188/200], Step [200/391] Loss: 0.0043\n",
            "Epoch [188/200], Step [300/391] Loss: 0.0075\n",
            "Epoch [189/200], Step [100/391] Loss: 0.0056\n",
            "Epoch [189/200], Step [200/391] Loss: 0.0152\n",
            "Epoch [189/200], Step [300/391] Loss: 0.0113\n",
            "Epoch [190/200], Step [100/391] Loss: 0.0087\n",
            "Epoch [190/200], Step [200/391] Loss: 0.0150\n",
            "Epoch [190/200], Step [300/391] Loss: 0.0257\n",
            "Accuracy of the model on the test images: 99.982 %\n",
            "Epoch [191/200], Step [100/391] Loss: 0.0046\n",
            "Epoch [191/200], Step [200/391] Loss: 0.0021\n",
            "Epoch [191/200], Step [300/391] Loss: 0.0030\n",
            "Epoch [192/200], Step [100/391] Loss: 0.0040\n",
            "Epoch [192/200], Step [200/391] Loss: 0.0125\n",
            "Epoch [192/200], Step [300/391] Loss: 0.0014\n",
            "Epoch [193/200], Step [100/391] Loss: 0.0015\n",
            "Epoch [193/200], Step [200/391] Loss: 0.0071\n",
            "Epoch [193/200], Step [300/391] Loss: 0.0125\n",
            "Epoch [194/200], Step [100/391] Loss: 0.0046\n",
            "Epoch [194/200], Step [200/391] Loss: 0.0152\n",
            "Epoch [194/200], Step [300/391] Loss: 0.0068\n",
            "Epoch [195/200], Step [100/391] Loss: 0.0026\n",
            "Epoch [195/200], Step [200/391] Loss: 0.0009\n",
            "Epoch [195/200], Step [300/391] Loss: 0.0062\n",
            "Epoch [196/200], Step [100/391] Loss: 0.0047\n",
            "Epoch [196/200], Step [200/391] Loss: 0.0044\n",
            "Epoch [196/200], Step [300/391] Loss: 0.0004\n",
            "Epoch [197/200], Step [100/391] Loss: 0.0014\n",
            "Epoch [197/200], Step [200/391] Loss: 0.0052\n",
            "Epoch [197/200], Step [300/391] Loss: 0.0032\n",
            "Epoch [198/200], Step [100/391] Loss: 0.0011\n",
            "Epoch [198/200], Step [200/391] Loss: 0.0003\n",
            "Epoch [198/200], Step [300/391] Loss: 0.0007\n",
            "Epoch [199/200], Step [100/391] Loss: 0.0072\n",
            "Epoch [199/200], Step [200/391] Loss: 0.0059\n",
            "Epoch [199/200], Step [300/391] Loss: 0.0030\n",
            "Epoch [200/200], Step [100/391] Loss: 0.0460\n",
            "Epoch [200/200], Step [200/391] Loss: 0.0015\n",
            "Epoch [200/200], Step [300/391] Loss: 0.0041\n",
            "Accuracy of the model on the test images: 99.992 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAU9r-FsYa-D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PATH = './drive/My Drive/COLAB/ELNetV2_200EPOCH.pth'\n",
        "torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss,\n",
        "            }, PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}